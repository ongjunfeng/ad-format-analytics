{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from config import IG_REEL_SCRAPING_CONFIG, COLUMN_MAPPER\n",
    "from apify_service import ApifyService\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fcd24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access job parameter dataset id. By default, this would not be set meaning job runs will scrape new datasets by default. If provided, the pipeline will work on past scrape jobs that \n",
    "dataset_id_param = dbutils.widgets.get(\"dataset_id\")  # will be empty string if not provided\n",
    "\n",
    "if dataset_id_param:\n",
    "    logger.info(f\"Using manually provided dataset_id: {dataset_id_param}\")\n",
    "    dataset_id = dataset_id_param\n",
    "else:\n",
    "    logger.info(\"No dataset_id provided. Using one generated by ApifyService.\")\n",
    "    dataset_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Scrape Reels data\n",
    "apify_service = ApifyService()\n",
    "if dataset_id:\n",
    "    scraped_df = apify_service.get_dataset_id_df(dataset_id)\n",
    "else:\n",
    "    scraped_df = apify_service.get_config_df(IG_REEL_SCRAPING_CONFIG, wait_secs=60*60)\n",
    "    dataset_id = scraped_df.loc[0, \"dataset_id\"]\n",
    "\n",
    "# Step 2: Clean and rename columns\n",
    "# Drop unneeded columns and map column names to work with downstream transformation functions\n",
    "try:\n",
    "    logger.info(\"Dropping unnecessary columns...\")\n",
    "    keep_columns_list = list(COLUMN_MAPPER.keys())\n",
    "    cleaned_df = scraped_df[keep_columns_list]\n",
    "    num_dropped_cols = scraped_df.shape[1] - len(keep_columns_list)\n",
    "    logger.info(f\"{num_dropped_cols} columns dropped.\")\n",
    "    logger.info(f\"{cleaned_df.shape[1]} columns kept.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to drop columns from scraped dataframe due to exception: {e}\")\n",
    "    raise\n",
    "# This will work if the previous try block worked\n",
    "logger.info(\"Mapping remaining columns...\")\n",
    "cleaned_df = cleaned_df.rename(COLUMN_MAPPER, axis=1)\n",
    "logger.info(\"Remaining columns mapped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf1991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame and save to Delta\n",
    "spark_df = spark.createDataFrame(cleaned_df)\n",
    "bronze_path = \"workspace.test.nus_bronze_instagram_ingested\"\n",
    "spark_df.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_path)\n",
    "\n",
    "# Register the dataset_id so next task can use it\n",
    "dbutils.jobs.taskValues.set(key=\"dataset_id\", value=dataset_id)\n",
    "dbutils.jobs.taskValues.set(key=\"bronze_path\", value=bronze_path)\n",
    "\n",
    "logger.info(f\"Bronze table written to {bronze_path} for dataset_id={dataset_id}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
