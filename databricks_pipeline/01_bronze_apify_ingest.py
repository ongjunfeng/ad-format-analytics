# Databricks notebook source
import logging
from config import IG_REEL_SCRAPING_CONFIG
from apify_service import ApifyService
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Rename mapping for apify output - This is critical to ensure functions refer to the correct columns
COLUMN_MAPPER = {
    "dataset_id": "dataset_id",
    "ingested_epoch": "ingested_epoch",
    "ingested_timestamp": "ingested_timestamp",
    "shortCode": "post_id",
    "videoViewCount": "views",
    "likesCount": "likes",
    "commentsCount": "comments",
    "videoDuration": "duration",
    "caption": "caption",
    "timestamp": "date",
    "url": "post_url",
    "videoUrl": "media_url",
    "ownerUsername": "username"
}

# COMMAND ----------

# Access job parameter dataset id. By default, this would not be set meaning job runs will scrape new datasets by default. If provided, the pipeline will work on past scrape jobs that
dbutils.widgets.text("dataset_id", "")
dataset_id_param = dbutils.widgets.get("dataset_id")  # will be empty string if not provided

if dataset_id_param:
    logger.info(f"Using manually provided dataset_id: {dataset_id_param}")
    dataset_id = dataset_id_param
else:
    logger.info("No dataset_id provided. Using one generated by ApifyService.")
    dataset_id = None

# COMMAND ----------

# Step 1: Scrape Reels data
dataset_id = "bp3Seq5reRhcf1rPh"
apify_service = ApifyService()
if dataset_id:
    scraped_df = apify_service.get_dataset_id_df(dataset_id)
else:
    scraped_df = apify_service.get_config_df(IG_REEL_SCRAPING_CONFIG, wait_secs=60*60)
    dataset_id = scraped_df.loc[0, "dataset_id"]

# Step 2: Clean and rename columns
# Drop unneeded columns and map column names to work with downstream transformation functions
try:
    logger.info("Dropping unnecessary columns...")
    keep_columns_list = list(COLUMN_MAPPER.keys())
    cleaned_df = scraped_df[keep_columns_list]
    num_dropped_cols = scraped_df.shape[1] - len(keep_columns_list)
    logger.info(f"{num_dropped_cols} columns dropped.")
    logger.info(f"{cleaned_df.shape[1]} columns kept.")
except Exception as e:
    logger.error(f"Failed to drop columns from scraped dataframe due to exception: {e}")
    raise
# This will work if the previous try block worked
logger.info("Mapping remaining columns...")
cleaned_df = cleaned_df.rename(COLUMN_MAPPER, axis=1)
logger.info("Remaining columns mapped.")
cleaned_df

# COMMAND ----------

from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType
)
from pyspark.sql.functions import col

# Target delta table schema
TARGET_SCHEMA = StructType([
    StructField("dataset_id", StringType(), False),
    StructField("ingested_epoch", DoubleType()),
    StructField("ingested_timestamp", StringType()),
    StructField("post_id", StringType()),
    StructField("views", DoubleType()),
    StructField("likes", DoubleType()),
    StructField("comments", DoubleType()),
    StructField("duration", DoubleType()),
    StructField("caption", StringType()),
    StructField("date", StringType()),
    StructField("post_url", StringType()),
    StructField("media_url", StringType()),
    StructField("username", StringType())
])

def enforce_schema(df, schema):
    """Casts and selects columns in the DataFrame to match the target schema."""
    select_cols = [
        col(field.name).cast(field.dataType).alias(field.name)
        for field in schema.fields
    ]
    return df.select(*select_cols)

# COMMAND ----------

# Convert to Spark DataFrame and save to Delta
spark_df = spark.createDataFrame(cleaned_df)

# Enforce schema and write to target Delta table
bronze_df = enforce_schema(spark_df, TARGET_SCHEMA)
bronze_path = "workspace.test.nus_bronze_instagram_ingested"
bronze_df.write.format("delta").mode("append").saveAsTable(bronze_path)

# Register the dataset_id so next task can use it
dbutils.jobs.taskValues.set(key="dataset_id", value=dataset_id)
dbutils.jobs.taskValues.set(key="bronze_path", value=bronze_path)

logger.info(f"Bronze table written to {bronze_path} for dataset_id={dataset_id}")